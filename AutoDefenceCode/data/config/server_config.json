{
  "host": "0.0.0.0",
  "port": 9006,
  "models": [
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/llama-2-13b-chat.Q8_0.gguf",
      "model_alias": "llama-2-13b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
      "model_alias": "tinyllama-1.1b",
      "chat_format": "zephyr",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 2048
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/llama-2-70b-chat.Q8_0.gguf",
      "model_alias": "llama-2-70b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/llama-2-7b-chat.Q8_0.gguf",
      "model_alias": "llama-2-7b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/llama-pro-8b-instruct.Q8_0.gguf",
      "model_alias": "llama-pro-8b",
      "chat_format": "zephyr",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/vicuna-13b-v1.3.0.ggmlv3.q8_0.gguf",
      "model_alias": "vicuna-13b-v1.3.0",
      "chat_format": "vicuna",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/vicuna-13b-v1.5-16k.Q8_0.gguf",
      "model_alias": "vicuna-13b-v1.5",
      "chat_format": "vicuna",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 16384
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/vicuna-33b.Q8_0.gguf",
      "model_alias": "vicuna-33b",
      "chat_format": "vicuna",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/vicuna-7b-v1.5-16k.Q8_0.gguf",
      "model_alias": "vicuna-7b-v1.5",
      "chat_format": "vicuna",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 16384
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/toxicqa-llama2-13b.Q8_0.gguf",
      "model_alias": "toxicqa-llama2-13b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/llamaguard-7b.Q8_0.gguf",
      "model_alias": "llamaguard-7b",
      "chat_format": "llama-2",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 2048
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/mistral-7b-instruct-v0.2.Q8_0.gguf",
      "model_alias": "mistral-7b-v0.2",
      "chat_format": "mistrallite",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 32768
    },
    {
      "model": "/nfs/hpc/dgx2-3/llama-cpp-models/mixtral-8x7b-instruct-v0.1.Q8_0.gguf",
      "model_alias": "mixtral-8x7b-v0.1",
      "chat_format": "mistrallite",
      "n_gpu_layers": -1,
      "offload_kqv": true,
      "n_threads": 32,
      "n_batch": 512,
      "n_ctx": 4096
    }
  ]
}